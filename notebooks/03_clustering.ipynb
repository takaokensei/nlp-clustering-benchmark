{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Clustering e M√©tricas\n",
        "\n",
        "Este notebook aplica os algoritmos de clustering e calcula todas as m√©tricas de avalia√ß√£o.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Adicionar src ao path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "from src.config import (\n",
        "    EMBEDDINGS_DIR, CLUSTERING_CONFIGS, N_CLUSTERS, RANDOM_STATE, FIGURES_DIR, PCA_N_COMPONENTS, TIMEOUT_SECONDS\n",
        ")\n",
        "from src.utils import (\n",
        "    load_embedding, compute_all_metrics, create_results_dataframe,\n",
        "    save_results_table, TABLES_DIR, apply_pca, append_result_to_csv, load_checkpoint_results\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carregar Embeddings e Labels Verdadeiros\n",
        "\n",
        "Execute os notebooks anteriores primeiro!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Carregando labels verdadeiros...\n",
            "   ‚úÖ 20NG-6: 5906 documentos, 6 classes\n",
            "   ‚úÖ PT-6: 315 documentos, 6 classes\n",
            "\n",
            "‚úÖ Labels verdadeiros carregados!\n"
          ]
        }
      ],
      "source": [
        "# Carregar dados e labels verdadeiros\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from src.config import RAW_DATA_DIR, TWENTY_NG_CATEGORIES, PT6_CLASS_COLUMN_CANDIDATES\n",
        "from src.utils import detect_class_column\n",
        "\n",
        "# Carregar 20NG-6 para obter labels\n",
        "print(\"üì• Carregando labels verdadeiros...\")\n",
        "newsgroups = fetch_20newsgroups(\n",
        "    subset='all',\n",
        "    categories=TWENTY_NG_CATEGORIES,\n",
        "    remove=('headers', 'footers', 'quotes'),\n",
        "    shuffle=True,\n",
        "    random_state=42\n",
        ")\n",
        "y_true_20ng = newsgroups.target\n",
        "print(f\"   ‚úÖ 20NG-6: {len(y_true_20ng)} documentos, {len(np.unique(y_true_20ng))} classes\")\n",
        "\n",
        "# Carregar PT-6 para obter labels\n",
        "pt6_file = RAW_DATA_DIR / \"pt6_preprocessed.csv\"\n",
        "if pt6_file.exists():\n",
        "    df_pt6 = pd.read_csv(pt6_file, encoding='utf-8-sig')\n",
        "    class_col = detect_class_column(df_pt6, PT6_CLASS_COLUMN_CANDIDATES)\n",
        "    \n",
        "    if class_col:\n",
        "        # Converter classes textuais para num√©ricas\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        le = LabelEncoder()\n",
        "        y_true_pt6 = le.fit_transform(df_pt6[class_col])\n",
        "        print(f\"   ‚úÖ PT-6: {len(y_true_pt6)} documentos, {len(np.unique(y_true_pt6))} classes\")\n",
        "    else:\n",
        "        raise ValueError(\"N√£o foi poss√≠vel detectar coluna de classe no PT-6\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Arquivo n√£o encontrado: {pt6_file}\")\n",
        "\n",
        "print(\"\\n‚úÖ Labels verdadeiros carregados!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Definir Fun√ß√µes de Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Fun√ß√µes de clustering definidas!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import warnings\n",
        "# Suprimir aviso de sintaxe do hdbscan (problema conhecido na biblioteca)\n",
        "warnings.filterwarnings('ignore', category=SyntaxWarning, module='hdbscan')\n",
        "import hdbscan\n",
        "\n",
        "def apply_kmeans(X, config):\n",
        "    \"\"\"Aplica K-Means clustering.\"\"\"\n",
        "    kmeans = KMeans(**config)\n",
        "    return kmeans.fit_predict(X)\n",
        "\n",
        "def apply_gmm(X, config):\n",
        "    \"\"\"Aplica Gaussian Mixture Model clustering.\"\"\"\n",
        "    gmm = GaussianMixture(**config)\n",
        "    return gmm.fit_predict(X)\n",
        "\n",
        "def apply_agglomerative(X, config):\n",
        "    \"\"\"Aplica Agglomerative Clustering.\"\"\"\n",
        "    agg = AgglomerativeClustering(**config)\n",
        "    return agg.fit_predict(X)\n",
        "\n",
        "def find_optimal_eps(X, k=4, min_samples=5):\n",
        "    \"\"\"Encontra eps √≥timo para DBSCAN usando k-distance graph.\"\"\"\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    neighbors = NearestNeighbors(n_neighbors=k)\n",
        "    neighbors_fit = neighbors.fit(X)\n",
        "    distances, indices = neighbors_fit.kneighbors(X)\n",
        "    distances = np.sort(distances, axis=0)\n",
        "    distances = distances[:, k-1]\n",
        "    # Usar o \"cotovelo\" da curva k-distance\n",
        "    # Uma heur√≠stica simples: usar percentil 90\n",
        "    eps = np.percentile(distances, 90)\n",
        "    return eps\n",
        "\n",
        "def apply_dbscan(X, config, optimize_eps=True):\n",
        "    \"\"\"Aplica DBSCAN clustering com otimiza√ß√£o de eps.\"\"\"\n",
        "    config = config.copy()\n",
        "    if optimize_eps:\n",
        "        optimal_eps = find_optimal_eps(X, k=4, min_samples=config.get('min_samples', 5))\n",
        "        config['eps'] = optimal_eps\n",
        "        print(f\"      üìä eps otimizado: {optimal_eps:.4f}\")\n",
        "    dbscan = DBSCAN(**config)\n",
        "    return dbscan.fit_predict(X)\n",
        "\n",
        "def apply_spectral(X, config):\n",
        "    \"\"\"Aplica Spectral Clustering.\"\"\"\n",
        "    spectral = SpectralClustering(**config)\n",
        "    return spectral.fit_predict(X)\n",
        "\n",
        "def apply_hdbscan(X, config):\n",
        "    \"\"\"Aplica HDBSCAN clustering.\"\"\"\n",
        "    clusterer = hdbscan.HDBSCAN(**config)\n",
        "    return clusterer.fit_predict(X)\n",
        "\n",
        "# Mapeamento de algoritmos para fun√ß√µes\n",
        "CLUSTERING_FUNCTIONS = {\n",
        "    'kmeans': apply_kmeans,\n",
        "    'gmm': apply_gmm,\n",
        "    'agglomerative': apply_agglomerative,\n",
        "    'dbscan': apply_dbscan,\n",
        "    'spectral': apply_spectral,\n",
        "    'hdbscan': apply_hdbscan,\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Fun√ß√µes de clustering definidas!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Aplicar Clustering e Calcular M√©tricas\n",
        "\n",
        "Para cada combina√ß√£o de (dataset, embedding, algoritmo), aplicamos o clustering e calculamos todas as m√©tricas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CARREGANDO EMBEDDINGS\n",
            "============================================================\n",
            "\n",
            "üìä 20NG6:\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\20ng6_tfidf_svd.npy\n",
            "   ‚úÖ tfidf_svd: shape (5906, 300)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\20ng6_sbert.npy\n",
            "   ‚úÖ sbert: shape (5906, 768)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\20ng6_gte.npy\n",
            "   ‚úÖ gte: shape (5906, 768)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\20ng6_bge.npy\n",
            "   ‚úÖ bge: shape (5906, 1024)\n",
            "\n",
            "üìä PT6:\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\pt6_tfidf_svd.npy\n",
            "   ‚úÖ tfidf_svd: shape (315, 300)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\pt6_sbert.npy\n",
            "   ‚úÖ sbert: shape (315, 768)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\pt6_gte.npy\n",
            "   ‚úÖ gte: shape (315, 768)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\pt6_bge.npy\n",
            "   ‚úÖ bge: shape (315, 1024)\n",
            "\n",
            "============================================================\n",
            "APLICANDO CLUSTERING E CALCULANDO M√âTRICAS\n",
            "============================================================\n",
            "üîÑ Recuverados 0 resultados do checkpoint.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | tfidf_svd | gmm:   2%|‚ñè         | 1/48 [00:03<02:28,  3.17s/it]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   üîÑ Aplicando gmm em 20ng6/tfidf_svd... (pode demorar)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | tfidf_svd | dbscan:   6%|‚ñã         | 3/48 [01:16<18:16, 24.37s/it]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      üìä eps otimizado: 0.5220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | tfidf_svd | spectral:   8%|‚ñä         | 4/48 [01:16<11:02, 15.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   üîÑ Aplicando spectral em 20ng6/tfidf_svd... (pode demorar)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | tfidf_svd | hdbscan:  10%|‚ñà         | 5/48 [01:21<08:06, 11.32s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   üîÑ Aplicando hdbscan em 20ng6/tfidf_svd... (pode demorar)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | tfidf_svd | hdbscan:  12%|‚ñà‚ñé        | 6/48 [01:32<07:50, 11.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      üìâ Reduzindo dimensionalidade (PCA): 768 -> 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | sbert | gmm:  15%|‚ñà‚ñç        | 7/48 [01:33<05:19,  7.78s/it]        "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   üîÑ Aplicando gmm em 20ng6/sbert... (pode demorar)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | sbert | dbscan:  19%|‚ñà‚ñâ        | 9/48 [01:39<03:16,  5.04s/it]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      üìä eps otimizado: 2.1736\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | sbert | spectral:  21%|‚ñà‚ñà        | 10/48 [01:39<02:18,  3.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   üîÑ Aplicando spectral em 20ng6/sbert... (pode demorar)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | sbert | hdbscan:  23%|‚ñà‚ñà‚ñé       | 11/48 [01:44<02:28,  4.02s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   üîÑ Aplicando hdbscan em 20ng6/sbert... (pode demorar)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | gte | kmeans:  25%|‚ñà‚ñà‚ñå       | 12/48 [01:48<02:22,  3.96s/it]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      üìâ Reduzindo dimensionalidade (PCA): 768 -> 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | gte | gmm:  27%|‚ñà‚ñà‚ñã       | 13/48 [01:48<01:42,  2.93s/it]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   üîÑ Aplicando gmm em 20ng6/gte... (pode demorar)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | gte | dbscan:  31%|‚ñà‚ñà‚ñà‚ñè      | 15/48 [01:54<01:27,  2.66s/it]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      üìä eps otimizado: 0.3908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | gte | spectral:  33%|‚ñà‚ñà‚ñà‚ñé      | 16/48 [01:54<01:04,  2.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   üîÑ Aplicando spectral em 20ng6/gte... (pode demorar)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | gte | hdbscan:  35%|‚ñà‚ñà‚ñà‚ñå      | 17/48 [01:58<01:18,  2.54s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   üîÑ Aplicando hdbscan em 20ng6/gte... (pode demorar)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | gte | hdbscan:  38%|‚ñà‚ñà‚ñà‚ñä      | 18/48 [02:02<01:27,  2.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      üìâ Reduzindo dimensionalidade (PCA): 1024 -> 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | bge | gmm:  40%|‚ñà‚ñà‚ñà‚ñâ      | 19/48 [02:02<01:04,  2.23s/it]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   üîÑ Aplicando gmm em 20ng6/bge... (pode demorar)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | bge | dbscan:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/48 [02:16<01:50,  4.11s/it]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      üìä eps otimizado: 11.9770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | bge | spectral:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 22/48 [02:17<01:18,  3.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   üîÑ Aplicando spectral em 20ng6/bge... (pode demorar)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | bge | spectral:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 22/48 [18:11<21:30, 49.62s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     73\u001b[39m     y_pred = cluster_func(X, config, optimize_eps=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     y_pred = \u001b[43mcluster_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Calcular m√©tricas\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Nota: Silhouette √© otimizado para datasets grandes (usa amostra)\u001b[39;00m\n\u001b[32m     79\u001b[39m metrics = compute_all_metrics(y_true, y_pred, X)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mapply_spectral\u001b[39m\u001b[34m(X, config)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Aplica Spectral Clustering.\"\"\"\u001b[39;00m\n\u001b[32m     48\u001b[39m spectral = SpectralClustering(**config)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspectral\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\sklearn\\cluster\\_spectral.py:796\u001b[39m, in \u001b[36mSpectralClustering.fit_predict\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    775\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Perform spectral clustering on `X` and return cluster labels.\u001b[39;00m\n\u001b[32m    776\u001b[39m \n\u001b[32m    777\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    794\u001b[39m \u001b[33;03m        Cluster labels.\u001b[39;00m\n\u001b[32m    795\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\sklearn\\base.py:707\u001b[39m, in \u001b[36mClusterMixin.fit_predict\u001b[39m\u001b[34m(self, X, y, **kwargs)\u001b[39m\n\u001b[32m    684\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    685\u001b[39m \u001b[33;03mPerform clustering on `X` and returns cluster labels.\u001b[39;00m\n\u001b[32m    686\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    703\u001b[39m \u001b[33;03m    Cluster labels.\u001b[39;00m\n\u001b[32m    704\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[32m    706\u001b[39m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.labels_\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\sklearn\\cluster\\_spectral.py:748\u001b[39m, in \u001b[36mSpectralClustering.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    739\u001b[39m n_components = (\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_clusters \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_components\n\u001b[32m    741\u001b[39m )\n\u001b[32m    742\u001b[39m \u001b[38;5;66;03m# We now obtain the real valued solution matrix to the\u001b[39;00m\n\u001b[32m    743\u001b[39m \u001b[38;5;66;03m# relaxed Ncut problem, solving the eigenvalue problem\u001b[39;00m\n\u001b[32m    744\u001b[39m \u001b[38;5;66;03m# L_sym x = lambda x  and recovering u = D^-1/2 x.\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;66;03m# The first eigenvector is constant only for fully connected graphs\u001b[39;00m\n\u001b[32m    746\u001b[39m \u001b[38;5;66;03m# and should be kept for spectral clustering (drop_first = False)\u001b[39;00m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# See spectral_embedding documentation.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m maps = \u001b[43m_spectral_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maffinity_matrix_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m    \u001b[49m\u001b[43meigen_solver\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meigen_solver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[43meigen_tol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meigen_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n\u001b[32m    757\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mComputing label assignment using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.assign_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:366\u001b[39m, in \u001b[36m_spectral_embedding\u001b[39m\u001b[34m(adjacency, n_components, eigen_solver, random_state, eigen_tol, norm_laplacian, drop_first)\u001b[39m\n\u001b[32m    362\u001b[39m v0 = _init_arpack_v0(laplacian.shape[\u001b[32m0\u001b[39m], random_state)\n\u001b[32m    363\u001b[39m laplacian = check_array(\n\u001b[32m    364\u001b[39m     laplacian, accept_sparse=\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, accept_large_sparse=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    365\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m _, diffusion_map = \u001b[43meigsh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlaplacian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhich\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mv0\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m embedding = diffusion_map.T[n_components::-\u001b[32m1\u001b[39m]\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m norm_laplacian:\n\u001b[32m    371\u001b[39m     \u001b[38;5;66;03m# recover u = D^-1/2 x from the eigenvector output x\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\scipy\\sparse\\linalg\\_eigen\\arpack\\arpack.py:1704\u001b[39m, in \u001b[36meigsh\u001b[39m\u001b[34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[39m\n\u001b[32m   1702\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _ARPACK_LOCK:\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params.converged:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m         \u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1706\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m params.extract(return_eigenvectors)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\scipy\\sparse\\linalg\\_eigen\\arpack\\arpack.py:568\u001b[39m, in \u001b[36m_SymmetricArpackParams.iterate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    567\u001b[39m         Bxslice = \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28mself\u001b[39m.ipntr[\u001b[32m2\u001b[39m] - \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.ipntr[\u001b[32m2\u001b[39m] - \u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.n)\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m         \u001b[38;5;28mself\u001b[39m.workd[yslice] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mOPa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworkd\u001b[49m\u001b[43m[\u001b[49m\u001b[43mBxslice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ido == \u001b[32m2\u001b[39m:\n\u001b[32m    570\u001b[39m     \u001b[38;5;28mself\u001b[39m.workd[yslice] = \u001b[38;5;28mself\u001b[39m.B(\u001b[38;5;28mself\u001b[39m.workd[xslice])\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py:258\u001b[39m, in \u001b[36mLinearOperator.matvec\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.shape != (N,) \u001b[38;5;129;01mand\u001b[39;00m x.shape != (N,\u001b[32m1\u001b[39m):\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mdimension mismatch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_matvec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np.matrix):\n\u001b[32m    261\u001b[39m     y = asmatrix(y)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\scipy\\sparse\\linalg\\_eigen\\arpack\\arpack.py:945\u001b[39m, in \u001b[36mLuInv._matvec\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_matvec\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlu_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mM_lu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\scipy\\linalg\\_decomp_lu.py:185\u001b[39m, in \u001b[36mlu_solve\u001b[39m\u001b[34m(lu_and_piv, b, trans, overwrite_b, check_finite)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Solve an equation system, a x = b, given the LU factorization of a\u001b[39;00m\n\u001b[32m    138\u001b[39m \n\u001b[32m    139\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m (lu, piv) = lu_and_piv\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lu_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpiv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_b\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\scipy\\_lib\\_util.py:1233\u001b[39m, in \u001b[36m_apply_over_batch.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m \u001b[38;5;66;03m# Early exit if call is not batched\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(batch_shapes):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;66;03m# Determine broadcasted batch shape\u001b[39;00m\n\u001b[32m   1236\u001b[39m batch_shape = np.broadcast_shapes(*batch_shapes)  \u001b[38;5;66;03m# Gives OK error message\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp-clustering-benchmark\\.venv\\Lib\\site-packages\\scipy\\linalg\\_decomp_lu.py:207\u001b[39m, in \u001b[36m_lu_solve\u001b[39m\u001b[34m(lu, piv, b, trans, overwrite_b, check_finite)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.empty_like(b1, dtype=m.dtype)\n\u001b[32m    206\u001b[39m getrs, = get_lapack_funcs((\u001b[33m'\u001b[39m\u001b[33mgetrs\u001b[39m\u001b[33m'\u001b[39m,), (lu, b1))\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m x, info = \u001b[43mgetrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpiv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_b\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info == \u001b[32m0\u001b[39m:\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Configura√ß√µes\n",
        "datasets = {\n",
        "    '20ng6': {'embeddings': {}, 'labels': y_true_20ng},\n",
        "    'pt6': {'embeddings': {}, 'labels': y_true_pt6}\n",
        "}\n",
        "\n",
        "embedding_types = ['tfidf_svd', 'sbert', 'gte', 'bge']\n",
        "algorithms = list(CLUSTERING_CONFIGS.keys())\n",
        "\n",
        "# Carregar todos os embeddings\n",
        "print(\"=\" * 60)\n",
        "print(\"CARREGANDO EMBEDDINGS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for dataset_name in datasets.keys():\n",
        "    print(f\"\\nüìä {dataset_name.upper()}:\")\n",
        "    for emb_type in embedding_types:\n",
        "        embedding = load_embedding(dataset_name, emb_type, EMBEDDINGS_DIR)\n",
        "        if embedding is not None:\n",
        "            datasets[dataset_name]['embeddings'][emb_type] = embedding\n",
        "            print(f\"   ‚úÖ {emb_type}: shape {embedding.shape}\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå {emb_type}: n√£o encontrado\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"APLICANDO CLUSTERING E CALCULANDO M√âTRICAS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Arquivo de checkpoint\n",
        "CHECKPOINT_FILE = TABLES_DIR / \"clustering_results_checkpoint.csv\"\n",
        "\n",
        "# Carregar resultados j√° processados\n",
        "existing_results = load_checkpoint_results(CHECKPOINT_FILE)\n",
        "done_combinations = {(r['dataset'], r['embedding'], r['algorithm']) for r in existing_results}\n",
        "all_results = existing_results.copy()\n",
        "print(f\"üîÑ Recuverados {len(all_results)} resultados do checkpoint.\")\n",
        "\n",
        "# Iterar sobre todas as combina√ß√µes\n",
        "total_combinations = sum(\n",
        "    len(datasets[ds]['embeddings']) * len(algorithms)\n",
        "    for ds in datasets.keys()\n",
        ")\n",
        "\n",
        "with tqdm(total=total_combinations, desc=\"Processando\") as pbar:\n",
        "    for dataset_name, dataset_data in datasets.items():\n",
        "        y_true = dataset_data['labels']\n",
        "        \n",
        "        for emb_type, X_orig in dataset_data['embeddings'].items():\n",
        "            # Aplicar PCA se for embedding denso e n√£o for tfidf\n",
        "            if 'tfidf' not in emb_type:\n",
        "                X = apply_pca(X_orig, n_components=PCA_N_COMPONENTS)\n",
        "            else:\n",
        "                X = X_orig\n",
        "            \n",
        "            for algo_name in algorithms:\n",
        "                # Pular se j√° feito\n",
        "                if (dataset_name, emb_type, algo_name) in done_combinations:\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "\n",
        "                pbar.set_description(f\"{dataset_name} | {emb_type} | {algo_name}\")\n",
        "                \n",
        "                try:\n",
        "                    # Aplicar clustering\n",
        "                    config = CLUSTERING_CONFIGS[algo_name].copy()\n",
        "                    cluster_func = CLUSTERING_FUNCTIONS[algo_name]\n",
        "                    \n",
        "                    # Log para algoritmos mais lentos\n",
        "                    if algo_name in ['gmm', 'spectral', 'hdbscan']:\n",
        "                        print(f\"\\n   üîÑ Aplicando {algo_name} em {dataset_name}/{emb_type}... (pode demorar)\")\n",
        "                    \n",
        "                    if algo_name == 'dbscan':\n",
        "                        y_pred = cluster_func(X, config, optimize_eps=True)\n",
        "                    else:\n",
        "                        y_pred = cluster_func(X, config)\n",
        "                    \n",
        "                    # Calcular m√©tricas\n",
        "                    # Nota: Silhouette √© otimizado para datasets grandes (usa amostra)\n",
        "                    metrics = compute_all_metrics(y_true, y_pred, X)\n",
        "                    \n",
        "                    # Adicionar metadados\n",
        "                    result = {\n",
        "                        'dataset': dataset_name,\n",
        "                        'embedding': emb_type,\n",
        "                        'algorithm': algo_name,\n",
        "                        **metrics\n",
        "                    }\n",
        "                    \n",
        "                    # Adicionar informa√ß√µes sobre clusters\n",
        "                    # Para DBSCAN/HDBSCAN, -1 indica ru√≠do\n",
        "                    unique_labels = np.unique(y_pred)\n",
        "                    n_clusters = len(unique_labels[unique_labels >= 0])  # Ignorar ru√≠do (-1)\n",
        "                    n_noise = int(np.sum(y_pred == -1)) if -1 in unique_labels else 0\n",
        "                    result['n_clusters'] = n_clusters\n",
        "                    result['n_noise'] = n_noise\n",
        "                    \n",
        "                    all_results.append(result)\n",
        "                    append_result_to_csv(result, CHECKPOINT_FILE)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"\\n‚ö†Ô∏è Erro em {dataset_name} | {emb_type} | {algo_name}: {e}\")\n",
        "                    result = {\n",
        "                        'dataset': dataset_name,\n",
        "                        'embedding': emb_type,\n",
        "                        'algorithm': algo_name,\n",
        "                        'ari': np.nan,\n",
        "                        'nmi': np.nan,\n",
        "                        'purity': np.nan,\n",
        "                        'silhouette': np.nan,\n",
        "                        'n_clusters': np.nan,\n",
        "                        'n_noise': np.nan,\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "                    all_results.append(result)\n",
        "                \n",
        "                pbar.update(1)\n",
        "\n",
        "print(\"\\n‚úÖ Clustering conclu√≠do!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Criar Tabela de Resultados e Salvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar DataFrame com resultados\n",
        "results_df = create_results_dataframe(all_results)\n",
        "\n",
        "# Exibir resumo\n",
        "print(\"=\" * 60)\n",
        "print(\"RESUMO DOS RESULTADOS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTotal de combina√ß√µes: {len(results_df)}\")\n",
        "print(f\"\\nDatasets: {results_df['dataset'].unique()}\")\n",
        "print(f\"Embeddings: {results_df['embedding'].unique()}\")\n",
        "print(f\"Algoritmos: {results_df['algorithm'].unique()}\")\n",
        "\n",
        "# Exibir primeiras linhas\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PRIMEIRAS LINHAS DA TABELA\")\n",
        "print(\"=\" * 60)\n",
        "print(results_df.head(10).to_string())\n",
        "\n",
        "# Salvar tabela completa\n",
        "save_results_table(results_df, \"clustering_results\", TABLES_DIR)\n",
        "\n",
        "# Salvar tabelas separadas por dataset\n",
        "for dataset in results_df['dataset'].unique():\n",
        "    df_subset = results_df[results_df['dataset'] == dataset]\n",
        "    save_results_table(df_subset, f\"clustering_results_{dataset}\", TABLES_DIR)\n",
        "\n",
        "print(\"\\n‚úÖ Tabelas salvas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. An√°lise e Visualiza√ß√£o dos Resultados\n",
        "\n",
        "Visualiza√ß√£o r√°pida dos melhores resultados por m√©trica.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configurar estilo\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"Set2\")\n",
        "\n",
        "# Criar visualiza√ß√µes por m√©trica\n",
        "metrics_to_plot = ['ari', 'nmi', 'purity', 'silhouette']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Pivot table para heatmap\n",
        "    pivot_data = results_df.pivot_table(\n",
        "        values=metric,\n",
        "        index='embedding',\n",
        "        columns='algorithm',\n",
        "        aggfunc='mean'\n",
        "    )\n",
        "    \n",
        "    # Criar heatmap\n",
        "    sns.heatmap(\n",
        "        pivot_data,\n",
        "        annot=True,\n",
        "        fmt='.3f',\n",
        "        cmap='YlOrRd',\n",
        "        ax=ax,\n",
        "        cbar_kws={'label': metric.upper()}\n",
        "    )\n",
        "    \n",
        "    ax.set_title(f'{metric.upper()} - M√©dia entre Datasets', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Algoritmo', fontsize=10)\n",
        "    ax.set_ylabel('Embedding', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'clustering_metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"‚úÖ Heatmap salvo em: {FIGURES_DIR / 'clustering_metrics_heatmap.png'}\")\n",
        "plt.show()\n",
        "\n",
        "# Melhores resultados por m√©trica\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MELHORES RESULTADOS POR M√âTRICA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for metric in metrics_to_plot:\n",
        "    if metric in results_df.columns:\n",
        "        best = results_df.nlargest(3, metric)[['dataset', 'embedding', 'algorithm', metric]]\n",
        "        print(f\"\\nüèÜ Top 3 - {metric.upper()}:\")\n",
        "        print(best.to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
