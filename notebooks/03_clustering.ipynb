{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Clustering e M√©tricas\n",
        "\n",
        "Este notebook aplica os algoritmos de clustering e calcula todas as m√©tricas de avalia√ß√£o.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Adicionar src ao path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "from src.config import (\n",
        "    EMBEDDINGS_DIR, CLUSTERING_CONFIGS, N_CLUSTERS, RANDOM_STATE, FIGURES_DIR\n",
        ")\n",
        "from src.utils import (\n",
        "    load_embedding, compute_all_metrics, create_results_dataframe,\n",
        "    save_results_table, TABLES_DIR\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carregar Embeddings e Labels Verdadeiros\n",
        "\n",
        "Execute os notebooks anteriores primeiro!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Carregando labels verdadeiros...\n",
            "   ‚úÖ 20NG-6: 5906 documentos, 6 classes\n",
            "   ‚úÖ PT-6: 315 documentos, 6 classes\n",
            "\n",
            "‚úÖ Labels verdadeiros carregados!\n"
          ]
        }
      ],
      "source": [
        "# Carregar dados e labels verdadeiros\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from src.config import RAW_DATA_DIR, TWENTY_NG_CATEGORIES, PT6_CLASS_COLUMN_CANDIDATES\n",
        "from src.utils import detect_class_column\n",
        "\n",
        "# Carregar 20NG-6 para obter labels\n",
        "print(\"üì• Carregando labels verdadeiros...\")\n",
        "newsgroups = fetch_20newsgroups(\n",
        "    subset='all',\n",
        "    categories=TWENTY_NG_CATEGORIES,\n",
        "    remove=('headers', 'footers', 'quotes'),\n",
        "    shuffle=True,\n",
        "    random_state=42\n",
        ")\n",
        "y_true_20ng = newsgroups.target\n",
        "print(f\"   ‚úÖ 20NG-6: {len(y_true_20ng)} documentos, {len(np.unique(y_true_20ng))} classes\")\n",
        "\n",
        "# Carregar PT-6 para obter labels\n",
        "pt6_file = RAW_DATA_DIR / \"pt6_preprocessed.csv\"\n",
        "if pt6_file.exists():\n",
        "    df_pt6 = pd.read_csv(pt6_file, encoding='utf-8-sig')\n",
        "    class_col = detect_class_column(df_pt6, PT6_CLASS_COLUMN_CANDIDATES)\n",
        "    \n",
        "    if class_col:\n",
        "        # Converter classes textuais para num√©ricas\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        le = LabelEncoder()\n",
        "        y_true_pt6 = le.fit_transform(df_pt6[class_col])\n",
        "        print(f\"   ‚úÖ PT-6: {len(y_true_pt6)} documentos, {len(np.unique(y_true_pt6))} classes\")\n",
        "    else:\n",
        "        raise ValueError(\"N√£o foi poss√≠vel detectar coluna de classe no PT-6\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Arquivo n√£o encontrado: {pt6_file}\")\n",
        "\n",
        "print(\"\\n‚úÖ Labels verdadeiros carregados!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Definir Fun√ß√µes de Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Fun√ß√µes de clustering definidas!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import warnings\n",
        "# Suprimir aviso de sintaxe do hdbscan (problema conhecido na biblioteca)\n",
        "warnings.filterwarnings('ignore', category=SyntaxWarning, module='hdbscan')\n",
        "import hdbscan\n",
        "\n",
        "def apply_kmeans(X, config):\n",
        "    \"\"\"Aplica K-Means clustering.\"\"\"\n",
        "    kmeans = KMeans(**config)\n",
        "    return kmeans.fit_predict(X)\n",
        "\n",
        "def apply_gmm(X, config):\n",
        "    \"\"\"Aplica Gaussian Mixture Model clustering.\"\"\"\n",
        "    gmm = GaussianMixture(**config)\n",
        "    return gmm.fit_predict(X)\n",
        "\n",
        "def apply_agglomerative(X, config):\n",
        "    \"\"\"Aplica Agglomerative Clustering.\"\"\"\n",
        "    agg = AgglomerativeClustering(**config)\n",
        "    return agg.fit_predict(X)\n",
        "\n",
        "def find_optimal_eps(X, k=4, min_samples=5):\n",
        "    \"\"\"Encontra eps √≥timo para DBSCAN usando k-distance graph.\"\"\"\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    neighbors = NearestNeighbors(n_neighbors=k)\n",
        "    neighbors_fit = neighbors.fit(X)\n",
        "    distances, indices = neighbors_fit.kneighbors(X)\n",
        "    distances = np.sort(distances, axis=0)\n",
        "    distances = distances[:, k-1]\n",
        "    # Usar o \"cotovelo\" da curva k-distance\n",
        "    # Uma heur√≠stica simples: usar percentil 90\n",
        "    eps = np.percentile(distances, 90)\n",
        "    return eps\n",
        "\n",
        "def apply_dbscan(X, config, optimize_eps=True):\n",
        "    \"\"\"Aplica DBSCAN clustering com otimiza√ß√£o de eps.\"\"\"\n",
        "    config = config.copy()\n",
        "    if optimize_eps:\n",
        "        optimal_eps = find_optimal_eps(X, k=4, min_samples=config.get('min_samples', 5))\n",
        "        config['eps'] = optimal_eps\n",
        "        print(f\"      üìä eps otimizado: {optimal_eps:.4f}\")\n",
        "    dbscan = DBSCAN(**config)\n",
        "    return dbscan.fit_predict(X)\n",
        "\n",
        "def apply_spectral(X, config):\n",
        "    \"\"\"Aplica Spectral Clustering.\"\"\"\n",
        "    spectral = SpectralClustering(**config)\n",
        "    return spectral.fit_predict(X)\n",
        "\n",
        "def apply_hdbscan(X, config):\n",
        "    \"\"\"Aplica HDBSCAN clustering.\"\"\"\n",
        "    clusterer = hdbscan.HDBSCAN(**config)\n",
        "    return clusterer.fit_predict(X)\n",
        "\n",
        "# Mapeamento de algoritmos para fun√ß√µes\n",
        "CLUSTERING_FUNCTIONS = {\n",
        "    'kmeans': apply_kmeans,\n",
        "    'gmm': apply_gmm,\n",
        "    'agglomerative': apply_agglomerative,\n",
        "    'dbscan': apply_dbscan,\n",
        "    'spectral': apply_spectral,\n",
        "    'hdbscan': apply_hdbscan,\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Fun√ß√µes de clustering definidas!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Aplicar Clustering e Calcular M√©tricas\n",
        "\n",
        "Para cada combina√ß√£o de (dataset, embedding, algoritmo), aplicamos o clustering e calculamos todas as m√©tricas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CARREGANDO EMBEDDINGS\n",
            "============================================================\n",
            "\n",
            "üìä 20NG6:\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\20ng6_tfidf_svd.npy\n",
            "   ‚úÖ tfidf_svd: shape (5906, 300)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\20ng6_sbert.npy\n",
            "   ‚úÖ sbert: shape (5906, 768)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\20ng6_gte.npy\n",
            "   ‚úÖ gte: shape (5906, 768)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\20ng6_bge.npy\n",
            "   ‚úÖ bge: shape (5906, 1024)\n",
            "\n",
            "üìä PT6:\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\pt6_tfidf_svd.npy\n",
            "   ‚úÖ tfidf_svd: shape (315, 300)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\pt6_sbert.npy\n",
            "   ‚úÖ sbert: shape (315, 768)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\pt6_gte.npy\n",
            "   ‚úÖ gte: shape (315, 768)\n",
            "Embedding carregado de: C:\\nlp-clustering-benchmark\\data\\embeddings\\pt6_bge.npy\n",
            "   ‚úÖ bge: shape (315, 1024)\n",
            "\n",
            "============================================================\n",
            "APLICANDO CLUSTERING E CALCULANDO M√âTRICAS\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | tfidf_svd | dbscan:   6%|‚ñã         | 3/48 [01:12<17:27, 23.27s/it]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      üìä eps otimizado: 0.5220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | sbert | dbscan:  19%|‚ñà‚ñâ        | 9/48 [01:53<05:37,  8.65s/it]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      üìä eps otimizado: 2.6762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | gte | dbscan:  31%|‚ñà‚ñà‚ñà‚ñè      | 15/48 [02:49<04:54,  8.94s/it]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      üìä eps otimizado: 0.5902\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | bge | dbscan:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 21/48 [03:55<05:06, 11.35s/it]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      üìä eps otimizado: 16.6587\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20ng6 | bge | spectral:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 22/48 [03:57<03:39,  8.44s/it]"
          ]
        }
      ],
      "source": [
        "# Configura√ß√µes\n",
        "datasets = {\n",
        "    '20ng6': {'embeddings': {}, 'labels': y_true_20ng},\n",
        "    'pt6': {'embeddings': {}, 'labels': y_true_pt6}\n",
        "}\n",
        "\n",
        "embedding_types = ['tfidf_svd', 'sbert', 'gte', 'bge']\n",
        "algorithms = list(CLUSTERING_CONFIGS.keys())\n",
        "\n",
        "# Carregar todos os embeddings\n",
        "print(\"=\" * 60)\n",
        "print(\"CARREGANDO EMBEDDINGS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for dataset_name in datasets.keys():\n",
        "    print(f\"\\nüìä {dataset_name.upper()}:\")\n",
        "    for emb_type in embedding_types:\n",
        "        embedding = load_embedding(dataset_name, emb_type, EMBEDDINGS_DIR)\n",
        "        if embedding is not None:\n",
        "            datasets[dataset_name]['embeddings'][emb_type] = embedding\n",
        "            print(f\"   ‚úÖ {emb_type}: shape {embedding.shape}\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå {emb_type}: n√£o encontrado\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"APLICANDO CLUSTERING E CALCULANDO M√âTRICAS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Armazenar todos os resultados\n",
        "all_results = []\n",
        "\n",
        "# Iterar sobre todas as combina√ß√µes\n",
        "total_combinations = sum(\n",
        "    len(datasets[ds]['embeddings']) * len(algorithms)\n",
        "    for ds in datasets.keys()\n",
        ")\n",
        "\n",
        "with tqdm(total=total_combinations, desc=\"Processando\") as pbar:\n",
        "    for dataset_name, dataset_data in datasets.items():\n",
        "        y_true = dataset_data['labels']\n",
        "        \n",
        "        for emb_type, X in dataset_data['embeddings'].items():\n",
        "            for algo_name in algorithms:\n",
        "                pbar.set_description(f\"{dataset_name} | {emb_type} | {algo_name}\")\n",
        "                \n",
        "                try:\n",
        "                    # Aplicar clustering\n",
        "                    config = CLUSTERING_CONFIGS[algo_name].copy()\n",
        "                    cluster_func = CLUSTERING_FUNCTIONS[algo_name]\n",
        "                    \n",
        "                    # Log para algoritmos mais lentos\n",
        "                    if algo_name in ['gmm', 'spectral', 'hdbscan']:\n",
        "                        print(f\"\\n   üîÑ Aplicando {algo_name} em {dataset_name}/{emb_type}... (pode demorar)\")\n",
        "                    \n",
        "                    if algo_name == 'dbscan':\n",
        "                        y_pred = cluster_func(X, config, optimize_eps=True)\n",
        "                    else:\n",
        "                        y_pred = cluster_func(X, config)\n",
        "                    \n",
        "                    # Calcular m√©tricas\n",
        "                    # Nota: Silhouette √© otimizado para datasets grandes (usa amostra)\n",
        "                    metrics = compute_all_metrics(y_true, y_pred, X)\n",
        "                    \n",
        "                    # Adicionar metadados\n",
        "                    result = {\n",
        "                        'dataset': dataset_name,\n",
        "                        'embedding': emb_type,\n",
        "                        'algorithm': algo_name,\n",
        "                        **metrics\n",
        "                    }\n",
        "                    \n",
        "                    # Adicionar informa√ß√µes sobre clusters\n",
        "                    # Para DBSCAN/HDBSCAN, -1 indica ru√≠do\n",
        "                    unique_labels = np.unique(y_pred)\n",
        "                    n_clusters = len(unique_labels[unique_labels >= 0])  # Ignorar ru√≠do (-1)\n",
        "                    n_noise = int(np.sum(y_pred == -1)) if -1 in unique_labels else 0\n",
        "                    result['n_clusters'] = n_clusters\n",
        "                    result['n_noise'] = n_noise\n",
        "                    \n",
        "                    all_results.append(result)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"\\n‚ö†Ô∏è Erro em {dataset_name} | {emb_type} | {algo_name}: {e}\")\n",
        "                    result = {\n",
        "                        'dataset': dataset_name,\n",
        "                        'embedding': emb_type,\n",
        "                        'algorithm': algo_name,\n",
        "                        'ari': np.nan,\n",
        "                        'nmi': np.nan,\n",
        "                        'purity': np.nan,\n",
        "                        'silhouette': np.nan,\n",
        "                        'n_clusters': np.nan,\n",
        "                        'n_noise': np.nan,\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "                    all_results.append(result)\n",
        "                \n",
        "                pbar.update(1)\n",
        "\n",
        "print(\"\\n‚úÖ Clustering conclu√≠do!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Criar Tabela de Resultados e Salvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar DataFrame com resultados\n",
        "results_df = create_results_dataframe(all_results)\n",
        "\n",
        "# Exibir resumo\n",
        "print(\"=\" * 60)\n",
        "print(\"RESUMO DOS RESULTADOS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTotal de combina√ß√µes: {len(results_df)}\")\n",
        "print(f\"\\nDatasets: {results_df['dataset'].unique()}\")\n",
        "print(f\"Embeddings: {results_df['embedding'].unique()}\")\n",
        "print(f\"Algoritmos: {results_df['algorithm'].unique()}\")\n",
        "\n",
        "# Exibir primeiras linhas\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PRIMEIRAS LINHAS DA TABELA\")\n",
        "print(\"=\" * 60)\n",
        "print(results_df.head(10).to_string())\n",
        "\n",
        "# Salvar tabela completa\n",
        "save_results_table(results_df, \"clustering_results\", TABLES_DIR)\n",
        "\n",
        "# Salvar tabelas separadas por dataset\n",
        "for dataset in results_df['dataset'].unique():\n",
        "    df_subset = results_df[results_df['dataset'] == dataset]\n",
        "    save_results_table(df_subset, f\"clustering_results_{dataset}\", TABLES_DIR)\n",
        "\n",
        "print(\"\\n‚úÖ Tabelas salvas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. An√°lise e Visualiza√ß√£o dos Resultados\n",
        "\n",
        "Visualiza√ß√£o r√°pida dos melhores resultados por m√©trica.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configurar estilo\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"Set2\")\n",
        "\n",
        "# Criar visualiza√ß√µes por m√©trica\n",
        "metrics_to_plot = ['ari', 'nmi', 'purity', 'silhouette']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Pivot table para heatmap\n",
        "    pivot_data = results_df.pivot_table(\n",
        "        values=metric,\n",
        "        index='embedding',\n",
        "        columns='algorithm',\n",
        "        aggfunc='mean'\n",
        "    )\n",
        "    \n",
        "    # Criar heatmap\n",
        "    sns.heatmap(\n",
        "        pivot_data,\n",
        "        annot=True,\n",
        "        fmt='.3f',\n",
        "        cmap='YlOrRd',\n",
        "        ax=ax,\n",
        "        cbar_kws={'label': metric.upper()}\n",
        "    )\n",
        "    \n",
        "    ax.set_title(f'{metric.upper()} - M√©dia entre Datasets', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Algoritmo', fontsize=10)\n",
        "    ax.set_ylabel('Embedding', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'clustering_metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"‚úÖ Heatmap salvo em: {FIGURES_DIR / 'clustering_metrics_heatmap.png'}\")\n",
        "plt.show()\n",
        "\n",
        "# Melhores resultados por m√©trica\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MELHORES RESULTADOS POR M√âTRICA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for metric in metrics_to_plot:\n",
        "    if metric in results_df.columns:\n",
        "        best = results_df.nlargest(3, metric)[['dataset', 'embedding', 'algorithm', metric]]\n",
        "        print(f\"\\nüèÜ Top 3 - {metric.upper()}:\")\n",
        "        print(best.to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
