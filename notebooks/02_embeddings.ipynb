{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Gera√ß√£o de Embeddings\n",
        "\n",
        "Este notebook gera todos os embeddings (TF-IDF+SVD, SBERT, GTE, BGE) e os salva em cache.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU detectada: NVIDIA GeForce RTX 3060 Ti\n",
            "   Mem√≥ria total: 8.00 GB\n",
            "\n",
            "üìã Modelos configurados:\n",
            "   SBERT: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
            "   GTE: thenlper/gte-base\n",
            "   BGE: BAAI/bge-m3\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Adicionar src ao path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "from src.config import (\n",
        "    EMBEDDINGS_DIR, TFIDF_CONFIG, SVD_CONFIG, EMBEDDING_MODELS,\n",
        "    EMBEDDING_BATCH_SIZE, EMBEDDING_DEVICE, RANDOM_STATE\n",
        ")\n",
        "from src.utils import save_embedding, load_embedding\n",
        "\n",
        "# Recarregar m√≥dulo config para garantir que est√° usando a vers√£o mais recente\n",
        "import importlib\n",
        "import src.config\n",
        "importlib.reload(src.config)\n",
        "from src.config import EMBEDDING_MODELS\n",
        "\n",
        "# Verificar disponibilidade de GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Mem√≥ria total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPU n√£o detectada. Usando CPU.\")\n",
        "\n",
        "# Verificar modelos carregados\n",
        "print(f\"\\nüìã Modelos configurados:\")\n",
        "print(f\"   SBERT: {EMBEDDING_MODELS['sbert']}\")\n",
        "print(f\"   GTE: {EMBEDDING_MODELS['gte']}\")\n",
        "print(f\"   BGE: {EMBEDDING_MODELS['bge']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîë Token do Hugging Face encontrado no arquivo .env\n",
            "‚úÖ Login realizado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# Login no Hugging Face usando vari√°veis de ambiente do arquivo .env\n",
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Carregar vari√°veis de ambiente do arquivo .env\n",
        "load_dotenv()\n",
        "\n",
        "# Obter token do arquivo .env\n",
        "hf_token = os.getenv('HUGGING_FACE_TOKEN')\n",
        "\n",
        "if hf_token:\n",
        "    print(\"üîë Token do Hugging Face encontrado no arquivo .env\")\n",
        "    login(token=hf_token)\n",
        "    print(\"‚úÖ Login realizado com sucesso!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Token HUGGING_FACE_TOKEN n√£o encontrado no arquivo .env\")\n",
        "    print(\"üí° Adicione seu token no arquivo .env:\")\n",
        "    print(\"   HUGGING_FACE_TOKEN=seu_token_aqui\")\n",
        "    print(\"\\n   Ou fa√ßa login manualmente:\")\n",
        "    print(\"   login()  # Descomente esta linha e execute\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carregar Dados Preparados\n",
        "\n",
        "Execute o notebook 01_data_prep.ipynb primeiro para ter os dados dispon√≠veis!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Carregando dados...\n",
            "   Carregando 20NG-6...\n",
            "   ‚úÖ 20NG-6: 5906 documentos\n",
            "   Carregando PT-6 pr√©-processado...\n",
            "   ‚úÖ PT-6: 315 documentos (coluna: Categoria)\n",
            "‚úÖ Todos os dados foram carregados!\n",
            "\\nüìù Coluna de texto usada para PT-6: 'Texto Expandido'\n"
          ]
        }
      ],
      "source": [
        "# Carregar dados (recarrega automaticamente se necess√°rio)\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from src.config import RAW_DATA_DIR, TWENTY_NG_CATEGORIES, PT6_CLASS_COLUMN_CANDIDATES\n",
        "from src.utils import detect_class_column\n",
        "\n",
        "# Verificar se os DataFrames j√° est√£o dispon√≠veis\n",
        "if 'df_20ng' not in globals() or 'df_pt6' not in globals():\n",
        "    print(\"üì• Carregando dados...\")\n",
        "    \n",
        "    # Carregar 20NG-6\n",
        "    print(\"   Carregando 20NG-6...\")\n",
        "    newsgroups = fetch_20newsgroups(\n",
        "        subset='all',\n",
        "        categories=TWENTY_NG_CATEGORIES,\n",
        "        remove=('headers', 'footers', 'quotes'),\n",
        "        shuffle=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    df_20ng = pd.DataFrame({\n",
        "        'text': newsgroups.data,\n",
        "        'class': newsgroups.target,\n",
        "        'class_name': [newsgroups.target_names[i] for i in newsgroups.target]\n",
        "    })\n",
        "    print(f\"   ‚úÖ 20NG-6: {len(df_20ng)} documentos\")\n",
        "    \n",
        "    # Carregar PT-6 pr√©-processado\n",
        "    print(\"   Carregando PT-6 pr√©-processado...\")\n",
        "    pt6_preprocessed_file = RAW_DATA_DIR / \"pt6_preprocessed.csv\"\n",
        "    \n",
        "    if pt6_preprocessed_file.exists():\n",
        "        df_pt6 = pd.read_csv(pt6_preprocessed_file, encoding='utf-8-sig')\n",
        "        class_col = detect_class_column(df_pt6, PT6_CLASS_COLUMN_CANDIDATES)\n",
        "        \n",
        "        # Verifica√ß√£o de seguran√ßa: garantir que n√£o h√° NaN na coluna de texto\n",
        "        text_col_check = 'Texto Expandido' if 'Texto Expandido' in df_pt6.columns else 'Texto Original'\n",
        "        nan_count = df_pt6[text_col_check].isna().sum()\n",
        "        if nan_count > 0:\n",
        "            print(f\"   ‚ö†Ô∏è Encontrados {nan_count} valores NaN. Removendo...\")\n",
        "            df_pt6 = df_pt6[df_pt6[text_col_check].notna()].reset_index(drop=True)\n",
        "            print(f\"   ‚úÖ Ap√≥s limpeza: {len(df_pt6)} documentos\")\n",
        "        \n",
        "        print(f\"   ‚úÖ PT-6: {len(df_pt6)} documentos (coluna: {class_col})\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Arquivo pr√©-processado n√£o encontrado: {pt6_preprocessed_file}\\\\n\"\n",
        "            \"Execute o notebook 01_data_prep.ipynb primeiro para gerar o CSV pr√©-processado!\"\n",
        "        )\n",
        "    \n",
        "    print(\"‚úÖ Todos os dados foram carregados!\")\n",
        "else:\n",
        "    print(\"‚úÖ Dados j√° est√£o dispon√≠veis na sess√£o!\")\n",
        "    # Usar globals().get() para evitar avisos do linter\n",
        "    df_20ng_check = globals().get('df_20ng')\n",
        "    df_pt6_check = globals().get('df_pt6')\n",
        "    if df_20ng_check is not None:\n",
        "        print(f\"   20NG-6: {len(df_20ng_check)} documentos\")\n",
        "    if df_pt6_check is not None:\n",
        "        print(f\"   PT-6: {len(df_pt6_check)} documentos\")\n",
        "\n",
        "# Garantir que as vari√°veis est√£o no escopo local\n",
        "if 'df_20ng' not in locals():\n",
        "    df_20ng = globals().get('df_20ng')\n",
        "if 'df_pt6' not in locals():\n",
        "    df_pt6 = globals().get('df_pt6')\n",
        "\n",
        "# Definir coluna de texto para PT-6 (usar 'Texto Expandido' se dispon√≠vel)\n",
        "if df_pt6 is not None:\n",
        "    text_col_pt6 = 'Texto Expandido' if 'Texto Expandido' in df_pt6.columns else 'Texto Original'\n",
        "    print(f\"\\\\nüìù Coluna de texto usada para PT-6: '{text_col_pt6}'\")\n",
        "else:\n",
        "    raise RuntimeError(\"df_pt6 n√£o est√° dispon√≠vel. Execute a c√©lula anterior primeiro.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. TF-IDF + SVD (Baseline Lexical)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def build_tfidf_svd_embeddings(texts, dataset_name):\n",
        "    \"\"\"Gera embeddings TF-IDF + SVD.\"\"\"\n",
        "    # Verificar cache\n",
        "    cached = load_embedding(dataset_name, 'tfidf_svd', EMBEDDINGS_DIR)\n",
        "    if cached is not None:\n",
        "        print(f\"‚úÖ Embedding TF-IDF+SVD j√° existe para {dataset_name}\")\n",
        "        return cached\n",
        "    \n",
        "    # TF-IDF\n",
        "    print(f\"\\\\nüî® Gerando TF-IDF para {dataset_name}...\")\n",
        "    vectorizer = TfidfVectorizer(**TFIDF_CONFIG)\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    print(f\"   TF-IDF shape: {tfidf_matrix.shape}\")\n",
        "    \n",
        "    # SVD\n",
        "    print(f\"   Aplicando SVD para reduzir a {SVD_CONFIG['n_components']} dimens√µes...\")\n",
        "    svd = TruncatedSVD(**SVD_CONFIG)\n",
        "    embeddings = svd.fit_transform(tfidf_matrix)\n",
        "    print(f\"   ‚úÖ Embeddings finais shape: {embeddings.shape}\")\n",
        "    \n",
        "    # Salvar\n",
        "    save_embedding(embeddings, dataset_name, 'tfidf_svd', EMBEDDINGS_DIR)\n",
        "    return embeddings\n",
        "\n",
        "# Garantir que text_col_pt6 est√° definida\n",
        "if 'text_col_pt6' not in globals():\n",
        "    text_col_pt6 = 'Texto Expandido' if 'Texto Expandido' in df_pt6.columns else 'Texto Original'\n",
        "    print(f\"üìù Coluna de texto para PT-6: '{text_col_pt6}'\")\n",
        "\n",
        "# Gerar para 20NG-6\n",
        "print(\"=\" * 60)\n",
        "print(\"GERANDO EMBEDDINGS TF-IDF+SVD\")\n",
        "print(\"=\" * 60)\n",
        "embeddings_20ng_tfidf = build_tfidf_svd_embeddings(df_20ng['text'].tolist(), '20ng6')\n",
        "\n",
        "# Gerar para PT-6 (dados j√° est√£o pr√©-processados)\n",
        "# Verifica√ß√£o final antes de gerar embeddings\n",
        "texts_pt6 = df_pt6[text_col_pt6].tolist()\n",
        "# Garantir que n√£o h√° NaN\n",
        "texts_pt6 = [str(t).strip() for t in texts_pt6 if pd.notna(t) and str(t).strip() != '' and str(t) != 'nan']\n",
        "print(f\"üìä PT-6: {len(texts_pt6)} textos v√°lidos para processar\")\n",
        "embeddings_pt6_tfidf = build_tfidf_svd_embeddings(texts_pt6, 'pt6')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. SBERT (Sentence Transformer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "def build_sbert_embeddings(texts, dataset_name):\n",
        "    \"\"\"Gera embeddings SBERT.\"\"\"\n",
        "    # Verificar cache\n",
        "    cached = load_embedding(dataset_name, 'sbert', EMBEDDINGS_DIR)\n",
        "    if cached is not None:\n",
        "        print(f\"‚úÖ Embedding SBERT j√° existe para {dataset_name}\")\n",
        "        return cached\n",
        "    \n",
        "    print(f\"\\\\nüî® Carregando modelo SBERT para {dataset_name}...\")\n",
        "    model = SentenceTransformer(EMBEDDING_MODELS['sbert'])\n",
        "    \n",
        "    # Usar GPU se dispon√≠vel\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # Aumentar batch_size para GPU (RTX 3060 Ti tem 8GB VRAM)\n",
        "    batch_size = 128 if device == 'cuda' else EMBEDDING_BATCH_SIZE\n",
        "    print(f\"   Gerando embeddings (batch_size={batch_size}, device={device})...\")\n",
        "    embeddings = model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        device=device,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "    \n",
        "    print(f\"   ‚úÖ Embeddings shape: {embeddings.shape}\")\n",
        "    \n",
        "    # Salvar\n",
        "    save_embedding(embeddings, dataset_name, 'sbert', EMBEDDINGS_DIR)\n",
        "    return embeddings\n",
        "\n",
        "# Gerar para 20NG-6\n",
        "print(\"=\" * 60)\n",
        "print(\"GERANDO EMBEDDINGS SBERT\")\n",
        "print(\"=\" * 60)\n",
        "embeddings_20ng_sbert = build_sbert_embeddings(df_20ng['text'].tolist(), '20ng6')\n",
        "\n",
        "# Gerar para PT-6 (dados j√° est√£o pr√©-processados)\n",
        "embeddings_pt6_sbert = build_sbert_embeddings(df_pt6[text_col_pt6].tolist(), 'pt6')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GTE (General Text Embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_gte_embeddings(texts, dataset_name):\n",
        "    \"\"\"Gera embeddings GTE.\"\"\"\n",
        "    # Verificar cache\n",
        "    cached = load_embedding(dataset_name, 'gte', EMBEDDINGS_DIR)\n",
        "    if cached is not None:\n",
        "        print(f\"‚úÖ Embedding GTE j√° existe para {dataset_name}\")\n",
        "        return cached\n",
        "    \n",
        "    print(f\"\\\\nüî® Carregando modelo GTE para {dataset_name}...\")\n",
        "    model = SentenceTransformer(EMBEDDING_MODELS['gte'])\n",
        "    \n",
        "    # Usar GPU se dispon√≠vel\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # Aumentar batch_size para GPU (RTX 3060 Ti tem 8GB VRAM)\n",
        "    batch_size = 128 if device == 'cuda' else EMBEDDING_BATCH_SIZE\n",
        "    print(f\"   Gerando embeddings (batch_size={batch_size}, device={device})...\")\n",
        "    embeddings = model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        device=device,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "    \n",
        "    print(f\"   ‚úÖ Embeddings shape: {embeddings.shape}\")\n",
        "    \n",
        "    # Salvar\n",
        "    save_embedding(embeddings, dataset_name, 'gte', EMBEDDINGS_DIR)\n",
        "    return embeddings\n",
        "\n",
        "# Gerar para 20NG-6\n",
        "print(\"=\" * 60)\n",
        "print(\"GERANDO EMBEDDINGS GTE\")\n",
        "print(\"=\" * 60)\n",
        "embeddings_20ng_gte = build_gte_embeddings(df_20ng['text'].tolist(), '20ng6')\n",
        "\n",
        "# Gerar para PT-6 (dados j√° est√£o pr√©-processados)\n",
        "embeddings_pt6_gte = build_gte_embeddings(df_pt6[text_col_pt6].tolist(), 'pt6')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. BGE (BAAI General Embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "GERANDO EMBEDDINGS BGE\n",
            "============================================================\n",
            "Embedding n√£o encontrado: C:\\nlp-clustering-benchmark\\data\\embeddings\\20ng6_bge.npy\n",
            "\\nüî® Carregando modelo BGE para 20ng6...\n",
            "   üì• Baixando APENAS safetensors (evitando .bin)...\n",
            "   üóëÔ∏è Cache BGE removido (ser√° baixado apenas safetensors)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a41073d26a4a4436873750f4f686d0e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 29 files:   0%|          | 0/29 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9279923e37541bca1e507f0cc87cd50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1846c9b83ea949c7938936a585210bd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c6c98a1d118476787483276ccfca667",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".gitattributes: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94a404f9aeaa4f64ae11f9e8dcdb1c29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cef285c7baf4a6f9fb55a28b20b7ea8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93624e1d5a46496b8ee33162976c92db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".DS_Store:   0%|          | 0.00/6.15k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ee7b6f053b84d29b534452b47b4c450",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "bm25.jpg:   0%|          | 0.00/132k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "932a3d3052294e3db71124e47a1e3fc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "colbert_linear.pt:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd575f6bd45c4ce2b14df8693e182402",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "long.jpg:   0%|          | 0.00/485k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91a3cf1befe14ed987529b82cee570e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb14aa4023774773aa9850e44614a97f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "long.jpg:   0%|          | 0.00/127k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12127fb3bf3a4d089929225f20f01e1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "mkqa.jpg:   0%|          | 0.00/608k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0207065ad314643a413b3226a15c7f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "nqa.jpg:   0%|          | 0.00/158k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3409d772a8684ac9873aa143b2bc8438",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "others.webp:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f56934126784485f8550e00d4ecfa695",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Constant_7_attr__value:   0%|          | 0.00/65.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71a83bb7ea45444e969578dc61759733",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "miracl.jpg:   0%|          | 0.00/576k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef8541b97ecf4855b97ec532ada80736",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff036a11ec3748928aca6d64964f2c20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.onnx:   0%|          | 0.00/725k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fcea12837fe4283abeebf3c983c45a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.onnx_data:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69e2f3ecf9d0426c9cf9f480b66cde65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fd6627b81b24c29aea43c2d49112875",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50007e8a241c49798f289fe417969640",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96039b8c9e154ac2acbd9242d1f3fe50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3030441b8794a6e86970fcb7baf6a6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f717cf852dff4030a7f217ecc14e90e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sparse_linear.pt:   0%|          | 0.00/3.52k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c443b50ce5744c79bba0f23bc250e054",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29d74505bd40421093c0eccfd2289068",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b09d44636ad4a5eb81ae71a2ad90b99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "047a04d35ef44bbab025bc6bbe4e95be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Modelo baixado (apenas safetensors)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d58b7820ffc4431f8488084859e5ef9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings (batch_size=128, device=cuda)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae6f22c8dbfb43c2bcbeaa05ca2f077e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/47 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Embeddings shape: (5906, 1024)\n",
            "Embedding salvo em: C:\\nlp-clustering-benchmark\\data\\embeddings\\20ng6_bge.npy\n",
            "Embedding n√£o encontrado: C:\\nlp-clustering-benchmark\\data\\embeddings\\pt6_bge.npy\n",
            "\\nüî® Carregando modelo BGE para pt6...\n",
            "   üì• Baixando APENAS safetensors (evitando .bin)...\n",
            "   üóëÔ∏è Cache BGE removido (ser√° baixado apenas safetensors)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a5fa5d000f342e29c18e772a9da37e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 29 files:   0%|          | 0/29 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e922dff099cb47718c49ceb9f737996d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97cf14f8e7a540daa5f1f056416be091",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46017a6f477244e59832b30c3d499914",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18fbf571d4be4bb3aff438f75e99aad8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".gitattributes: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e8010c927f24b0793d7388603391a07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".DS_Store:   0%|          | 0.00/6.15k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "640515fa64894aea92602e1e9968cf67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb284b4d1b3948128c40d0cc543591cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "miracl.jpg:   0%|          | 0.00/576k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8d74f78e49040688804bccc2c2dcaa8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "long.jpg:   0%|          | 0.00/485k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06e5bd340cf4480688326c83d2d69baa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "bm25.jpg:   0%|          | 0.00/132k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c84e64d2d2cb4768951dac28ad8bd71f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "nqa.jpg:   0%|          | 0.00/158k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42aa86050c644bce9b667c1a4015934c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "colbert_linear.pt:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7425847e4b4e47a093bdd7f7cd78f811",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "mkqa.jpg:   0%|          | 0.00/608k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c55e0fa76afb4939b4e1cc3dcfecd6a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "long.jpg:   0%|          | 0.00/127k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad5f693aac764a45819195dd32015c40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "others.webp:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14c2340e543549b0b79a42fb57049a64",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec520f843845474d807432de8010be50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Constant_7_attr__value:   0%|          | 0.00/65.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b92361e61834ddf93d66760962c8659",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4180844d09a64a6d8bdadac4ca67f4a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb089140c8274bc098ec31b506c5c3ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b4ac626eb73485eaa847b595e467b12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adf2b3e414cc4504a91550e5bcf39640",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f0265072d8f4a868bd6fd18620513b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.onnx:   0%|          | 0.00/725k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a4f9598b2644da190a7344af5fdcf1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f550df30f95f46e796485030738ae36b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.onnx_data:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fea7574dc897476088437c10460f15a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1079537a77a49ada94df5f3f29d3a14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4aef585a1e04c2485deb3b4d7d72add",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a08752af8a5407590e6556e115d0cf6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sparse_linear.pt:   0%|          | 0.00/3.52k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "909d61bf39c64238a715fdd7b03185f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Modelo baixado (apenas safetensors)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46cfb748fb65458b920e0b6daadfcf7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings (batch_size=128, device=cuda)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cd71a689ee64101a38007cc460f15d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Embeddings shape: (315, 1024)\n",
            "Embedding salvo em: C:\\nlp-clustering-benchmark\\data\\embeddings\\pt6_bge.npy\n"
          ]
        }
      ],
      "source": [
        "def build_bge_embeddings(texts, dataset_name):\n",
        "    \"\"\"Gera embeddings BGE usando apenas safetensors.\"\"\"\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from transformers import AutoModel, AutoTokenizer\n",
        "    from sentence_transformers import models\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "    \n",
        "    # Verificar cache\n",
        "    cached = load_embedding(dataset_name, 'bge', EMBEDDINGS_DIR)\n",
        "    if cached is not None:\n",
        "        print(f\"‚úÖ Embedding BGE j√° existe para {dataset_name}\")\n",
        "        return cached\n",
        "    \n",
        "    print(f\"\\\\nüî® Carregando modelo BGE para {dataset_name}...\")\n",
        "    print(\"   üì• Baixando APENAS safetensors (evitando .bin)...\")\n",
        "    \n",
        "    cache_dir = Path.home() / '.cache' / 'huggingface' / 'hub'\n",
        "    bge_cache = cache_dir / 'models--BAAI--bge-m3'\n",
        "    \n",
        "    # DELETAR cache completo do BGE para come√ßar limpo\n",
        "    if bge_cache.exists():\n",
        "        import shutil\n",
        "        try:\n",
        "            shutil.rmtree(bge_cache)\n",
        "            print(\"   üóëÔ∏è Cache BGE removido (ser√° baixado apenas safetensors)\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è N√£o foi poss√≠vel remover cache completo: {e}\")\n",
        "            # Tentar remover apenas .bin\n",
        "            for bin_file in bge_cache.rglob('pytorch_model.bin'):\n",
        "                try:\n",
        "                    bin_file.unlink()\n",
        "                except:\n",
        "                    pass\n",
        "    \n",
        "    # Configurar ambiente\n",
        "    os.environ['SAFETENSORS_FAST_GPU'] = '1'\n",
        "    os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
        "    \n",
        "    model_name = EMBEDDING_MODELS['bge']\n",
        "    \n",
        "    # Usar snapshot_download com ignore_patterns para N√ÉO baixar .bin\n",
        "    from huggingface_hub import snapshot_download\n",
        "    \n",
        "    # Baixar modelo ignorando .bin files\n",
        "    try:\n",
        "        model_path = snapshot_download(\n",
        "            repo_id=model_name,\n",
        "            ignore_patterns=[\"*.bin\", \"pytorch_model.bin\"],\n",
        "            cache_dir=str(cache_dir)\n",
        "        )\n",
        "        print(\"   ‚úÖ Modelo baixado (apenas safetensors)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Erro: {e}\")\n",
        "        # Continuar mesmo assim\n",
        "    \n",
        "    # Carregar usando transformers - vai usar apenas safetensors (sem .bin dispon√≠vel)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        use_safetensors=True,\n",
        "        local_files_only=False\n",
        "    )\n",
        "    \n",
        "    model_auto = AutoModel.from_pretrained(\n",
        "        model_name,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=True,\n",
        "        local_files_only=False\n",
        "    )\n",
        "    \n",
        "    # Verificar e remover .bin se foi baixado (n√£o deveria, mas por seguran√ßa)\n",
        "    if bge_cache.exists():\n",
        "        for bin_file in bge_cache.rglob('pytorch_model.bin'):\n",
        "            try:\n",
        "                bin_file.unlink()\n",
        "                print(f\"   üóëÔ∏è Removido {bin_file.name} (n√£o deveria existir)\")\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    # Criar SentenceTransformer usando o modelo carregado\n",
        "    word_embedding = models.Transformer(\n",
        "        model_name,\n",
        "        max_seq_length=512,\n",
        "        model_args={'use_safetensors': True}\n",
        "    )\n",
        "    # Substituir o modelo interno pelo que carregamos\n",
        "    word_embedding.auto_model = model_auto\n",
        "    word_embedding.tokenizer = tokenizer\n",
        "    \n",
        "    pooling = models.Pooling(\n",
        "        word_embedding.get_word_embedding_dimension(),\n",
        "        pooling_mode_mean_tokens=True\n",
        "    )\n",
        "    \n",
        "    model = SentenceTransformer(modules=[word_embedding, pooling])\n",
        "    \n",
        "    # Usar GPU se dispon√≠vel\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # Aumentar batch_size para GPU (RTX 3060 Ti tem 8GB VRAM)\n",
        "    batch_size = 128 if device == 'cuda' else EMBEDDING_BATCH_SIZE\n",
        "    print(f\"   Gerando embeddings (batch_size={batch_size}, device={device})...\")\n",
        "    embeddings = model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        device=device,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "    \n",
        "    print(f\"   ‚úÖ Embeddings shape: {embeddings.shape}\")\n",
        "    \n",
        "    # Salvar\n",
        "    save_embedding(embeddings, dataset_name, 'bge', EMBEDDINGS_DIR)\n",
        "    return embeddings\n",
        "\n",
        "# Gerar para 20NG-6\n",
        "print(\"=\" * 60)\n",
        "print(\"GERANDO EMBEDDINGS BGE\")\n",
        "print(\"=\" * 60)\n",
        "embeddings_20ng_bge = build_bge_embeddings(df_20ng['text'].tolist(), '20ng6')\n",
        "\n",
        "# Gerar para PT-6 (dados j√° est√£o pr√©-processados)\n",
        "embeddings_pt6_bge = build_bge_embeddings(df_pt6[text_col_pt6].tolist(), 'pt6')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Resumo e Verifica√ß√£o\n",
        "\n",
        "Verificar se todos os embeddings foram gerados e salvos corretamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "RESUMO DOS EMBEDDINGS GERADOS\n",
            "============================================================\n",
            "\n",
            "üìä 20NG6:\n",
            "   ‚úÖ tfidf_svd    - Shape: (5906, 300)\n",
            "   ‚úÖ sbert        - Shape: (5906, 768)\n",
            "   ‚úÖ gte          - Shape: (5906, 768)\n",
            "   ‚úÖ bge          - Shape: (5906, 1024)\n",
            "\n",
            "üìä PT6:\n",
            "   ‚úÖ tfidf_svd    - Shape: (315, 300)\n",
            "   ‚úÖ sbert        - Shape: (315, 768)\n",
            "   ‚úÖ gte          - Shape: (315, 768)\n",
            "   ‚úÖ bge          - Shape: (315, 1024)\n",
            "\n",
            "============================================================\n",
            "‚úÖ Todos os embeddings foram gerados e salvos em cache!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RESUMO DOS EMBEDDINGS GERADOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "datasets = ['20ng6', 'pt6']\n",
        "embedding_types = ['tfidf_svd', 'sbert', 'gte', 'bge']\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"\\nüìä {dataset.upper()}:\")\n",
        "    for emb_type in embedding_types:\n",
        "        filename = f\"{dataset}_{emb_type}.npy\"\n",
        "        filepath = EMBEDDINGS_DIR / filename\n",
        "        if filepath.exists():\n",
        "            emb = np.load(filepath)\n",
        "            print(f\"   ‚úÖ {emb_type:12s} - Shape: {emb.shape}\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå {emb_type:12s} - N√£o encontrado\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Todos os embeddings foram gerados e salvos em cache!\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
